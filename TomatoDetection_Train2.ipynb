{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8KCRZxrCCkoH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "import cv2 as cv\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import keras \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.resnet import ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lables</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12849</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12850</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12851</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12852</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12853</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12854 rows Ã— 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       lables\n",
              "0           1\n",
              "1           1\n",
              "2           1\n",
              "3           1\n",
              "4           1\n",
              "...       ...\n",
              "12849       0\n",
              "12850       0\n",
              "12851       0\n",
              "12852       0\n",
              "12853       0\n",
              "\n",
              "[12854 rows x 1 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels = pd.read_csv('labels3.csv')\n",
        "num = len(labels)\n",
        "labels = labels.iloc[: , 1:]\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = 'imcrop3/'\n",
        "imgs = []\n",
        "for i in range(num):\n",
        "    img = cv.imread(path + \"t{}.jpg\".format(i))\n",
        "    imgs.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12854"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "rand = random.randint(0,num)\n",
        "img_show = imgs[rand]\n",
        "plt.imshow(img_show)\n",
        "len(imgs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MfHlHXDWcogJ"
      },
      "outputs": [],
      "source": [
        "X_train, X_test , y_train, y_test = train_test_split(np.array(imgs),np.array(labels),test_size=0.10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(12854, 1)\n",
            "(11568, 224, 224, 3)\n",
            "(1286, 224, 224, 3)\n",
            "(11568, 1)\n",
            "(1286, 1)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(12854, 224, 224, 3)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print((np.array(labels)).shape)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "(np.array(imgs)).shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypxNxYjPdXOA",
        "outputId": "06396578-a9dc-47fd-f57f-5afa89436e13"
      },
      "outputs": [],
      "source": [
        "resnet = ResNet50(include_top = False, weights = 'imagenet', input_shape=(224,224,3), pooling = 'max')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUkXFk9m10U8",
        "outputId": "243ec385-b80a-4245-8e05-66fe4d5d8aed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<keras.engine.input_layer.InputLayer object at 0x00000241BC08B790>\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x000002427BBE2D00>\n",
            "<keras.layers.convolutional.Conv2D object at 0x000002427BC31340>\n",
            "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x000002427BC310A0>\n",
            "<keras.layers.core.Activation object at 0x000002427BC31BE0>\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x000002427BC31130>\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x000002427BCC9580>\n",
            "<keras.layers.convolutional.Conv2D object at 0x000002427BD72D90>\n",
            "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x000002427BD7E1F0>\n",
            "<keras.layers.core.Activation object at 0x000002427BD7ED60>\n",
            "<keras.layers.convolutional.Conv2D object at 0x000002427BD868E0>\n",
            "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x000002427BD89CD0>\n",
            "<keras.layers.core.Activation object at 0x000002427BD86D30>\n",
            "<keras.layers.convolutional.Conv2D object at 0x000002427BD67E80>\n",
            "<keras.layers.convolutional.Conv2D object at 0x000002427BD7A310>\n",
            "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x000002427BD67400>\n",
            "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x000002427BDA00A0>\n",
            "<keras.layers.merge.Add object at 0x000002427BDA0A90>\n",
            "<keras.layers.core.Activation object at 0x000002427BD8C130>\n",
            "<keras.layers.convolutional.Conv2D object at 0x000002427BDAB250>\n",
            "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x000002427BDABFD0>\n",
            "<keras.layers.core.Activation object at 0x000002427BDA45E0>\n",
            "<keras.layers.convolutional.Conv2D object at 0x000002427BDB1A60>\n",
            "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x000002427BDA0D90>\n",
            "<keras.layers.core.Activation object at 0x000002427BDB1EB0>\n",
            "<keras.layers.convolutional.Conv2D object at 0x000002427BD8C790>\n",
            "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x000002427BDC0C40>\n",
            "<keras.layers.merge.Add object at 0x000002427BDBCF40>\n",
            "<keras.layers.core.Activation object at 0x000002427BDB1460>\n",
            "<keras.layers.convolutional.Conv2D object at 0x000002427BDD63D0>\n"
          ]
        }
      ],
      "source": [
        "for layers in (resnet.layers)[:30]:\n",
        "    print(layers)\n",
        "    layers.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kRXGVXks23Js"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "from keras import regularizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "u0Eep1Vw3aM1"
      },
      "outputs": [],
      "source": [
        "model.add(resnet)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32,activation= tf.nn.relu,kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l1(0.01)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1,activation= tf.nn.sigmoid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5OtVdtv53_Np"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RhU7lyJC5PQQ"
      },
      "outputs": [],
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=True,\n",
        "    featurewise_std_normalization=True,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0NxE3FSh5Vqo"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "train_generator= datagen.flow(X_train, y_train, batch_size=BATCH_SIZE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "aUvUFp5h5fsD"
      },
      "outputs": [],
      "source": [
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqymm3qy5mcw",
        "outputId": "1a9b9a13-78de-415c-926b-55dcb69bba01"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(11568, 224, 224, 3)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('tomato_classifier_model.h5', verbose=1,\n",
        "                                                save_best_only=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "EPOCHS = 20\n",
        "model = keras.models.load_model(\"tomato_classifier_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVBw-lYj5omq",
        "outputId": "34521c7d-dcf9-472c-d665-9c5da8cf36c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\MaximusX\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
            "C:\\Users\\MaximusX\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "C:\\Users\\MaximusX\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "64/64 [==============================] - 58s 723ms/step - loss: 0.2739 - accuracy: 0.9236 - val_loss: 0.2470 - val_accuracy: 0.9253\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.24703, saving model to tomato_classifier_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\MaximusX\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  warnings.warn('Custom mask layers require a config and must override '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/20\n",
            "64/64 [==============================] - 49s 760ms/step - loss: 0.2689 - accuracy: 0.9228 - val_loss: 0.2814 - val_accuracy: 0.9215\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.24703\n",
            "Epoch 3/20\n",
            "64/64 [==============================] - 53s 829ms/step - loss: 0.2581 - accuracy: 0.9248 - val_loss: 0.2677 - val_accuracy: 0.9300\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.24703\n",
            "Epoch 4/20\n",
            "64/64 [==============================] - 50s 777ms/step - loss: 0.2659 - accuracy: 0.9238 - val_loss: 0.2289 - val_accuracy: 0.9370\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.24703 to 0.22891, saving model to tomato_classifier_model.h5\n",
            "Epoch 5/20\n",
            "64/64 [==============================] - 51s 785ms/step - loss: 0.2638 - accuracy: 0.9268 - val_loss: 0.2312 - val_accuracy: 0.9425\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.22891\n",
            "Epoch 6/20\n",
            "64/64 [==============================] - 50s 775ms/step - loss: 0.2528 - accuracy: 0.9289 - val_loss: 0.2323 - val_accuracy: 0.9362\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.22891\n",
            "Epoch 7/20\n",
            "64/64 [==============================] - 52s 807ms/step - loss: 0.2584 - accuracy: 0.9243 - val_loss: 0.2227 - val_accuracy: 0.9417\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.22891 to 0.22269, saving model to tomato_classifier_model.h5\n",
            "Epoch 8/20\n",
            "64/64 [==============================] - 52s 799ms/step - loss: 0.2495 - accuracy: 0.9324 - val_loss: 0.2184 - val_accuracy: 0.9401\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.22269 to 0.21837, saving model to tomato_classifier_model.h5\n",
            "Epoch 9/20\n",
            "64/64 [==============================] - 51s 784ms/step - loss: 0.2467 - accuracy: 0.9324 - val_loss: 0.2218 - val_accuracy: 0.9432\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.21837\n",
            "Epoch 10/20\n",
            "64/64 [==============================] - 50s 775ms/step - loss: 0.3286 - accuracy: 0.9306 - val_loss: 0.3095 - val_accuracy: 0.9020\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.21837\n",
            "Epoch 11/20\n",
            "64/64 [==============================] - 51s 785ms/step - loss: 0.2642 - accuracy: 0.9248 - val_loss: 0.2775 - val_accuracy: 0.9184\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.21837\n",
            "Epoch 12/20\n",
            "64/64 [==============================] - 51s 783ms/step - loss: 0.2639 - accuracy: 0.9140 - val_loss: 0.2821 - val_accuracy: 0.8733\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.21837\n",
            "Epoch 13/20\n",
            "64/64 [==============================] - 51s 797ms/step - loss: 0.2771 - accuracy: 0.9172 - val_loss: 0.3044 - val_accuracy: 0.8904\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.21837\n",
            "Epoch 14/20\n",
            "64/64 [==============================] - 52s 799ms/step - loss: 0.2452 - accuracy: 0.9221 - val_loss: 0.2213 - val_accuracy: 0.9409\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.21837\n",
            "Epoch 15/20\n",
            "64/64 [==============================] - 54s 830ms/step - loss: 0.2543 - accuracy: 0.9194 - val_loss: 0.2591 - val_accuracy: 0.9261\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.21837\n",
            "Epoch 16/20\n",
            "64/64 [==============================] - 53s 820ms/step - loss: 0.2217 - accuracy: 0.9385 - val_loss: 0.1967 - val_accuracy: 0.9549\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.21837 to 0.19671, saving model to tomato_classifier_model.h5\n",
            "Epoch 17/20\n",
            "64/64 [==============================] - 52s 801ms/step - loss: 0.2317 - accuracy: 0.9277 - val_loss: 0.2416 - val_accuracy: 0.9331\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.19671\n",
            "Epoch 18/20\n",
            "64/64 [==============================] - 55s 849ms/step - loss: 0.2449 - accuracy: 0.9297 - val_loss: 0.4123 - val_accuracy: 0.9261\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.19671\n",
            "Epoch 19/20\n",
            "64/64 [==============================] - 53s 816ms/step - loss: 0.2729 - accuracy: 0.9025 - val_loss: 1.1262 - val_accuracy: 0.7076\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.19671\n",
            "Epoch 20/20\n",
            "64/64 [==============================] - 50s 776ms/step - loss: 0.3382 - accuracy: 0.8962 - val_loss: 0.7452 - val_accuracy: 0.4572\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.19671\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x24282622ee0>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit_generator(train_generator, validation_data= (X_test, y_test),steps_per_epoch=BATCH_SIZE ,epochs=EPOCHS,callbacks=[checkpoint])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "A-w57USnpjDo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 45.72%\n"
          ]
        }
      ],
      "source": [
        "model.save_weights('model_weights.h5')\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_model = keras.models.load_model(\"tomato_classifier_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 95.49%\n"
          ]
        }
      ],
      "source": [
        "scores = new_model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "TomatoDetection_Zoe",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
